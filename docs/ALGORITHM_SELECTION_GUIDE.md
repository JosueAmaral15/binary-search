# Algorithm Selection Guide

**How to choose the right optimizer for your problem**

---

## ğŸ¯ The Golden Rule

### Is your problem LINEAR?

```
Can you write it as: Ax = b?
Examples: Linear regression, systems of equations
```

**YES â†’ Use NumPy Direct Solve** âœ…  
**NO â†’ Use BinaryRateOptimizer** âœ…

---

## ğŸ“Š Detailed Decision Tree

```
START
  â†“
Is it a LINEAR system (Ax = b)?
  â”œâ”€ YES â†’ Use np.linalg.solve()  [FASTEST - 10-1000Ã— speedup]
  â”‚         Examples:
  â”‚         â€¢ Linear regression
  â”‚         â€¢ Normal equations
  â”‚         â€¢ Matrix equations
  â”‚
  â””â”€ NO â†’ Is it gradient descent optimization?
      â”œâ”€ YES â†’ How many parameters?
      â”‚    â”œâ”€ < 10,000 â†’ BinaryRateOptimizer  [10Ã— faster than AdamW]
      â”‚    â””â”€ > 10,000 â†’ Consider AdamW or NumPy if reformulable
      â”‚
      â””â”€ NO â†’ Is it array search?
           â””â”€ Use BinarySearch (search algorithms)
```

---

## 1ï¸âƒ£ LINEAR Problems: Use NumPy Direct Solve

### What is a Linear Problem?

Any problem that can be written as **Ax = b** where:
- **A** is a matrix (coefficients)
- **x** is the unknown vector (variables to find)
- **b** is the result vector

### Examples:

#### Linear Regression
```python
import numpy as np

# Problem: Find Î¸ such that y â‰ˆ XÎ¸
# Solution: Normal equations â†’ (X^T X)Î¸ = X^T y

A = X.T @ X
b = X.T @ y
theta = np.linalg.solve(A, b)  # FASTEST!
```

#### System of Equations
```python
# Solve:
#   2x + 3y = 8
#   4x + 5y = 14

A = np.array([[2, 3],
              [4, 5]])
b = np.array([8, 14])

solution = np.linalg.solve(A, b)
# Result: x=1, y=2
```

#### Many Variables (x, y, z, w, a1, a2, ..., a500)
```python
# Even with 500+ variables, NumPy is FASTEST

# Generate problem: 1000 samples Ã— 500 variables
X = np.random.randn(1000, 500)
y = np.random.randn(1000)

# Solve in ~10 milliseconds
A = X.T @ X
b = X.T @ y
theta = np.linalg.solve(A, b)  # 500 coefficients found!
```

### Performance

| Variables | NumPy Time | BinaryRate Time | AdamW Time | Speedup |
|-----------|------------|-----------------|------------|---------|
| 5 | 0.1 ms | 3.7 ms | 6.7 ms | **57Ã—** |
| 10 | 0.1 ms | 5.3 ms | 7.2 ms | **89Ã—** |
| 50 | 0.2 ms | 10.4 ms | 11.0 ms | **62Ã—** |
| 100 | 0.3 ms | 20.8 ms | 13.5 ms | **46Ã—** |
| 200 | 1.1 ms | 107.9 ms | 19.0 ms | **18Ã—** |
| 500 | 10.1 ms | 250.7 ms | 103.8 ms | **10Ã—** |

**Conclusion:** NumPy is 10-90Ã— faster, always wins! âœ…

---

## 2ï¸âƒ£ NON-LINEAR Problems: Use BinaryRateOptimizer

### What is a Non-Linear Problem?

Any optimization problem that **cannot** be written as Ax = b:
- Custom cost functions
- Logistic regression
- Neural networks
- Non-convex optimization
- Regularized problems (beyond simple L2)

### Examples:

#### Logistic Regression
```python
from math_toolkit.optimization import BinaryRateOptimizer
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def cost(theta, X, y):
    h = sigmoid(X @ theta)
    return -np.mean(y * np.log(h) + (1-y) * np.log(1-h))

def gradient(theta, X, y):
    h = sigmoid(X @ theta)
    return X.T @ (h - y) / len(y)

# Optimize
optimizer = BinaryRateOptimizer(max_iter=50, tol=1e-6)
theta = optimizer.optimize(X, y, initial_theta, cost, gradient)
```

#### Custom Cost Function
```python
from math_toolkit.optimization import BinaryRateOptimizer

# Non-linear cost: minimize sum of absolute errors + penalty
def custom_cost(theta, X, y):
    predictions = X @ theta
    mae = np.mean(np.abs(predictions - y))
    penalty = 0.1 * np.sum(theta**4)  # Non-linear penalty
    return mae + penalty

def custom_gradient(theta, X, y):
    predictions = X @ theta
    errors = predictions - y
    grad_mae = X.T @ np.sign(errors) / len(y)
    grad_penalty = 0.4 * theta**3
    return grad_mae + grad_penalty

optimizer = BinaryRateOptimizer(max_iter=100)
theta = optimizer.optimize(X, y, initial_theta, custom_cost, custom_gradient)
```

#### Neural Network Training
```python
from math_toolkit.optimization import BinaryRateOptimizer

# Simple 2-layer network
def forward(theta, X):
    W1, b1, W2, b2 = split_theta(theta)
    hidden = np.tanh(X @ W1 + b1)
    output = hidden @ W2 + b2
    return output

def mse_cost(theta, X, y):
    return np.mean((forward(theta, X) - y) ** 2)

def compute_gradient(theta, X, y):
    # Backpropagation...
    return grad

optimizer = BinaryRateOptimizer(max_iter=200)
theta = optimizer.optimize(X, y, initial_theta, mse_cost, compute_gradient)
```

### Performance

**BinaryRateOptimizer vs AdamW** (1000 samples Ã— 50 features):

| Metric | BinaryRateOptimizer | AdamW | Winner |
|--------|---------------------|-------|--------|
| Time | 0.027s | 0.265s | **Binary 10Ã— faster** âœ… |
| Cost | 0.00462 | 0.00486 | **Binary better** âœ… |
| Iterations | 10 | 100 | **Binary 10Ã— fewer** âœ… |

---

## 3ï¸âƒ£ Alternative: AdamW (For Specific Cases)

### When to Use AdamW

```python
from math_toolkit.optimization import AdamW

optimizer = AdamW(use_binary_search=True, max_iter=100)
theta = optimizer.optimize(X, y, initial_theta, cost, gradient)
```

**Use AdamW when:**
- ğŸ”§ Integrating with PyTorch/TensorFlow frameworks
- ğŸ›ï¸ Need per-parameter adaptive learning rates
- ğŸ“Š Small to medium datasets
- ğŸ§ª Experimenting with different optimizers

**Don't use when:**
- âš ï¸ Large datasets (BinaryRateOptimizer is 10Ã— faster)
- âš ï¸ Linear problems (NumPy is 100Ã— faster)

---

## 4ï¸âƒ£ BinaryGaussSeidel (Iterative Linear Solver)

### When to Use

```python
from math_toolkit.linear_systems import BinaryGaussSeidel

solver = BinaryGaussSeidel(max_iterations=1000, tolerance=1e-6)
x = solver.solve(A, b)
```

**Use ONLY when:**
- âœ… Matrix A is **sparse** (mostly zeros)
- âœ… Matrix A is **strictly diagonally dominant**
- âœ… Want iterative solver (not direct)

**Don't use when:**
- âŒ Dense matrices (NumPy is faster)
- âŒ Not diagonally dominant (won't converge)
- âŒ Need guaranteed solution (use NumPy)

**Reality:** For most cases, `np.linalg.solve()` is better!

---

## 5ï¸âƒ£ BinarySearch (Array Search & Root Finding)

### When to Use

```python
from math_toolkit.optimization import BinarySearch

# Find value in sorted array
index = BinarySearch.search([1, 2, 3, 4, 5], target=3)

# Find root: x^2 = 100
result = BinarySearch.search_for_function(
    y=100,
    function=lambda x: x**2,
    tolerance=1e-6
)
```

**Use when:**
- ğŸ” Searching sorted arrays
- ğŸ¯ Finding function roots
- ğŸ“ Inverse function evaluation
- âš–ï¸ Tolerance-based comparisons

---

## ğŸ“‹ Quick Reference Table

| Problem Type | Best Algorithm | Why | Speedup |
|-------------|----------------|-----|---------|
| **Linear regression** | `np.linalg.solve()` | Exact, O(nÂ³), LAPACK optimized | 10-1000Ã— |
| **Systems of equations (Ax=b)** | `np.linalg.solve()` | Direct solver, guaranteed solution | 10-1000Ã— |
| **Many variables (linear)** | `np.linalg.solve()` | Scales well, still fastest | 10-90Ã— |
| **Logistic regression** | `BinaryRateOptimizer` | Non-linear, binary search LR | 10Ã— vs AdamW |
| **Neural networks** | `BinaryRateOptimizer` | Fast convergence, fewer iterations | 10Ã— vs AdamW |
| **Custom cost functions** | `BinaryRateOptimizer` | Adaptive learning rate | 10Ã— vs AdamW |
| **Deep learning frameworks** | `AdamW` | Per-parameter rates, PyTorch compatible | Industry standard |
| **Sparse linear systems** | `BinaryGaussSeidel` | Iterative, memory efficient | Use case specific |
| **Array search** | `BinarySearch` | O(log n) search | Standard algorithm |
| **Root finding** | `BinarySearch` | Function inversion | Standard algorithm |

---

## ğŸ“ Key Insights from Benchmarks

### 1. Linear Problems: NumPy Dominates

```
Problem: 1000 samples Ã— 200 variables (linear regression)

NumPy solve:         1.1 ms  â† WINNER
BinaryRateOptimizer: 107.9 ms
AdamW:               19.0 ms

Speedup: 18Ã— faster than BinaryRate, 17Ã— faster than AdamW
```

**Lesson:** If you can formulate as Ax = b, **always use NumPy**!

---

### 2. Non-Linear Problems: BinaryRateOptimizer Wins

```
Problem: 1000 samples Ã— 50 features (gradient descent)

BinaryRateOptimizer: 0.027s, 10 iterations  â† WINNER
AdamW:               0.265s, 100 iterations

Speedup: 10Ã— faster, better accuracy
```

**Lesson:** Binary search for learning rate dramatically reduces iterations!

---

### 3. More Variables â‰  Different Winner

```
Variables:  5  â†’  500 (100Ã— increase)

NumPy:     0.1ms â†’ 10.1ms (100Ã— slower, still FASTEST)
BinaryRate: 3.7ms â†’ 250.7ms (68Ã— slower)
AdamW:     6.7ms â†’ 103.8ms (15Ã— slower)

NumPy still wins by 10Ã— at 500 variables!
```

**Lesson:** NumPy scales better than gradient descent!

---

### 4. Complexity Order Misleading

```
BinaryRateOptimizer: O(nÂ²) per iteration Ã— 10 iterations
AdamW:               O(n) per iteration Ã— 100 iterations

Result: BinaryRate is 10Ã— FASTER!
```

**Lesson:** Fewer smart iterations > many simple iterations!

---

## ğŸ’¡ Common Mistakes

### âŒ Mistake 1: Using gradient descent for linear regression

```python
# SLOW (0.265s for 1000Ã—50)
optimizer = AdamW()
theta = optimizer.optimize(X, y, initial_theta, mse_cost, gradient)
```

```python
# FAST (0.005s for 1000Ã—50) âœ…
theta = np.linalg.solve(X.T @ X, X.T @ y)
```

**Fix:** If linear, use NumPy!

---

### âŒ Mistake 2: Using NumPy for non-linear problems

```python
# WON'T WORK - logistic regression is non-linear
theta = np.linalg.solve(X.T @ X, X.T @ y)  # Wrong!
```

```python
# CORRECT âœ…
optimizer = BinaryRateOptimizer()
theta = optimizer.optimize(X, y, initial_theta, logistic_cost, logistic_grad)
```

**Fix:** Non-linear problems need gradient descent!

---

### âŒ Mistake 3: Using BinaryGaussSeidel for dense matrices

```python
# SLOW and may not converge
solver = BinaryGaussSeidel()
x = solver.solve(dense_matrix, b)
```

```python
# FAST and guaranteed âœ…
x = np.linalg.solve(dense_matrix, b)
```

**Fix:** BinaryGaussSeidel only for sparse, diagonally-dominant matrices!

---

## ğŸš€ Final Recommendations

### Default Choice
1. **Try NumPy first** - Can you formulate as Ax = b?
2. **Use BinaryRateOptimizer** - If non-linear
3. **Consider AdamW** - If integrating with frameworks

### Performance Priority
```python
# FASTEST to SLOWEST (for linear problems)
np.linalg.solve()          # 1st choice âœ…
BinaryRateOptimizer        # 2nd choice (if non-linear)
AdamW                       # 3rd choice (if needed)
```

### When in Doubt
**Linear?** â†’ NumPy  
**Non-linear?** â†’ BinaryRateOptimizer  
**Framework integration?** â†’ AdamW

---

## ğŸ“š See Also

- [SCALABILITY_BENCHMARK.md](SCALABILITY_BENCHMARK.md) - Detailed performance tests
- [OPTIMIZER_BENCHMARK.md](OPTIMIZER_BENCHMARK.md) - Original comparison
- [OBSERVER_ADAMW.md](OBSERVER_ADAMW.md) - Parallel hyperparameter tuning
- [README.md](../README.md) - Package overview

---

**Last Updated:** 2026-01-25  
**Benchmark Platform:** Python 3, NumPy, Standard CPU
